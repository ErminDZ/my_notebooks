{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afcb1c4f-cc78-4ac7-92a0-42fa1bdfaa72",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 01 Exercise: Writing a Simple Web Crawler\n",
    "\n",
    "1. Write a simple web scraper that can capture all images in a document (like: https://www.cphbusiness.dk/). \n",
    "2. Write to a .md file in a format so all images are shown when markdown is pasted and executed in a cell\n",
    "3. Extend the web scraper to find all links (`<a>` elements) in the document.\n",
    "4. If you have time, let the crawler find all links in the linked documents as well (so we get one more level of the hypertext graph). Use threads if helpfull.\n",
    "\n",
    "\n",
    "In case a page returns a status code, which is not `200` we just disregard this page. See https://en.wikipedia.org/wiki/List_of_HTTP_status_codes for more detailes on the various HTTP status codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a044ad6f-f689-4fc5-867d-c6de9f492665",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "scrape_links() missing 2 required positional arguments: 'from_url' and 'for_depth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m start_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.version2.dk/artikel/google-deepmind-vi-oeger-sikkerheden-mod-misbrug-sundhedsdata-1074452\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m link_dict \u001b[38;5;241m=\u001b[39m scrape_links(from_url\u001b[38;5;241m=\u001b[39mstart_url, for_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mscrape_links\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: scrape_links() missing 2 required positional arguments: 'from_url' and 'for_depth'"
     ]
    }
   ],
   "source": [
    "def scrape_links(from_url, for_depth, all_links={}):\n",
    "    # TODO: Implement code for websraper\n",
    "    \n",
    "    # return dict(key=url, value=list of outgoing urls)\n",
    "    pass\n",
    "\n",
    "\n",
    "start_url = 'https://www.version2.dk/artikel/google-deepmind-vi-oeger-sikkerheden-mod-misbrug-sundhedsdata-1074452'\n",
    "\n",
    "link_dict = scrape_links(from_url=start_url, for_depth=2)\n",
    "scrape_links()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f988b7ac-2fb8-40b4-bea0-638c8cf70551",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 01 Exercise with findall()\n",
    "In the following text find all the family names of everyone with first name Peter:\n",
    "\n",
    "\"Peter Hansen was meeting up with Jacob Fransen for a quick lunch, but first he had to go by Peter Beier to pick up some chocolate for his wife. Meanwhile Pastor Peter Jensen was going to church to give his sermon for the same 3 people in his parish. Those were Peter Kold and Henrik Halberg plus a third person who had recently moved here from Norway called Peter Harold\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63474126-3099-4f2b-a12d-f8f5c3bfca3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Peters: ['Peter Hansen', 'Peter Beier', 'Peter Jensen', 'Peter Kold', 'Peter Harold']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "first_name_Peter = re.compile(r'Peter \\w+')\n",
    "#first_name_Peter = re.compile(r'\\w+ Peter \\w+')\n",
    "\n",
    "\n",
    "all_peters = \"\"\"\n",
    "Peter Hansen was meeting up with Jacob Fransen for a quick lunch, but first he had to go by Peter Beier to pick up some chocolate for his wife. Meanwhile Pastor Peter Jensen was going to church to give his sermon for the same 3 people in his parish.Those were Peter Kold and Henrik Halberg plus a third person who had recently moved here from Norway called Peter Harold\n",
    "\"\"\"\n",
    "\n",
    "names = first_name_Peter.findall(all_peters)\n",
    "print('All Peters: {}'.format(names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d908b7-8de8-4c7a-9a8c-c555bbd56097",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 02 exercise\n",
    "\n",
    "We will play with the addresses from data/addresses.txt and the following regex patterns\n",
    "\n",
    "Write a regular expression, that you can use to create 5 lists with:\n",
    "\n",
    "  * all names in the list above\n",
    "  * all telephone numbers \n",
    "  * all zip codes\n",
    "  * all city names with corresponding zip code\n",
    "  * all street names\n",
    "  \n",
    "\n",
    "\n",
    "|No|**Symbol**|**Effect**|\n",
    "|--|--|--|\n",
    "|1|.|dot matches any character except newline|\n",
    "|2|\\w|matches any word character i.e alphanumeric (letters, digits) and underscore ( _ )|\n",
    "|3|\\W|matches non word characters|\n",
    "|4|\\d|matches a single digit|\n",
    "|5|\\D|matches a single character that is not a digit|\n",
    "|6|\\s|matches any white-spaces character like \\t and \\n|\n",
    "|7|\\S|matches single non white space character|\n",
    "|8|[abc]|matches single character in the set i.e either match a, b or c|\n",
    "|9|[^abc]|match a single character other than a, b and c|\n",
    "|10|[a-z]|match a single character in the range a to z.|\n",
    "|11|[a-zA-ZæøåÆØÅ]|match a single character in the range a-å or A-Å|\n",
    "|12|[0-9]|match a single character in the range 0-9|\n",
    "|13|^|match start at beginning of the string|\n",
    "|14|$|match start at end of the string|\n",
    "|15|+|matches one or more of the preceding character (greedy match).|\n",
    "|16|*|matches zero or more of the preceding character (greedy match).|\n",
    "|17|?|matches zero or one of the preceding character.|\n",
    "|18|*?|non-greedy matches zero or more|\n",
    "|19|??|non-greedy zero or one|\n",
    "|20|+?|non-greedy one or more|\n",
    "|21|\\||or|\n",
    "|22|`([\\w ]+)*`|one ore more words|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1f6b267-5007-4062-a5a0-aee16a91c02f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Henning Gamborg Møller\n",
      "A K Møller\n",
      "A Møller\n",
      "A Møller\n",
      "A Møller\n",
      "A Møller\n",
      "A Møller\n",
      "A Møller Andersen\n",
      "A Møller Jensen\n",
      "A Møller Sørensen\n",
      "A Porse Møller\n",
      "Aage Beck Møller\n",
      "Aage Bojsen-Møller\n",
      "Aage Christian Møller Andersen\n",
      "Aage Hansen Møller\n",
      "Aage Jan Møller\n",
      "Aage Karl Møller\n",
      "Aage Majbom Møller\n",
      "Aage Martin Møller\n",
      "Aage Møller\n",
      "Aage Møller\n",
      "Aage Møller\n",
      "Aage Møller\n",
      "Aage Møller\n",
      "Aage Møller\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "with open('./my_data/addresses.txt') as f:\n",
    "    addresses = f.read()\n",
    "    \n",
    "    all_names = re.compile(r'^A (\\w+).+|^Aage (\\w+).+')\n",
    "    for line in addresses.split('\\n'):\n",
    "        if all_names.match(line):\n",
    "            print(all_names.match(line).group())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "342fc448-4c67-45d6-9d42-fa9b1edc3142",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 69 03 76\n",
      "75 50 75 14\n",
      "86 45 44 36\n",
      "76 42 00 81\n",
      "86 13 22 99\n",
      "97 95 20 01\n",
      "74 74 36 62\n",
      "45 80 47 14\n",
      "60 94 39 04\n",
      "35 38 97 81\n",
      "86 94 66 60\n",
      "20 44 00 35\n",
      "55 81 46 76\n",
      "20 83 70 65\n",
      "62 24 10 81\n",
      "20 83 88 62\n",
      "51 15 15 66\n",
      "28 59 06 93\n",
      "21 48 73 79\n",
      "64 82 11 54\n",
      "40 10 80 76\n",
      "21 79 64 18\n",
      "23 66 57 00\n",
      "60 45 79 69\n",
      "29 68 03 25\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "with open('./my_data/addresses.txt') as f:\n",
    "    addresses = f.read()\n",
    "    \n",
    "    all_telephone_numbers= re.compile(r'\\d{2} \\d{2} \\d{2} \\d{2}')\n",
    "    for line in addresses.split('\\n'):\n",
    "        if all_telephone_numbers.match(line):\n",
    "            print(all_telephone_numbers.match(line).group())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "94624cbe-63f3-42ab-9378-c7a89b0545c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6760 \n",
      "3460 \n",
      "8920 \n",
      "7000 \n",
      "8000 \n",
      "7760 \n",
      "6240 \n",
      "2840 \n",
      "8210 \n",
      "2100 \n",
      "8464 \n",
      "6630 \n",
      "4780 \n",
      "9240 \n",
      "5762 \n",
      "2650 \n",
      "3300 \n",
      "5550 \n",
      "6051 \n",
      "5450 \n",
      "8586 \n",
      "7900 \n",
      "5200 \n",
      "9700 \n",
      "8370 \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "with open('./my_data/addresses.txt') as f:\n",
    "    addresses = f.read()\n",
    "    \n",
    "    all_zip_code= re.compile(r'\\d{4} ')\n",
    "    for line in addresses.split('\\n'):\n",
    "        if all_zip_code.match(line):\n",
    "            print(all_zip_code.match(line).group())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "53fdf622-1c26-43b0-87f0-6a1d55bcb0ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6760 Ribe\n",
      "3460 Birkerød\n",
      "8920 Randers NV\n",
      "7000 Fredericia\n",
      "8000 Aarhus C\n",
      "7760 Hurup Thy\n",
      "6240 Løgumkloster\n",
      "2840 Holte\n",
      "8210 Aarhus V\n",
      "2100 København Ø\n",
      "8464 Galten\n",
      "6630 Rødding\n",
      "4780 Stege\n",
      "9240 Nibe\n",
      "5762 Vester Skerninge\n",
      "2650 Hvidovre\n",
      "3300 Frederiksværk\n",
      "5550 Langeskov\n",
      "6051 Almind\n",
      "5450 Otterup\n",
      "8586 Ørum Djurs\n",
      "7900 Nykøbing M\n",
      "5200 Odense V\n",
      "9700 Brønderslev\n",
      "8370 Hadsten\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "with open('./my_data/addresses.txt') as f:\n",
    "    addresses = f.read()\n",
    "    \n",
    "    all_city_names_withzipcode= re.compile(r'\\d{4} ([\\w ]+)*')\n",
    "    for line in addresses.split('\\n'):\n",
    "        if all_city_names_withzipcode.match(line):\n",
    "            print(all_city_names_withzipcode.match(line).group())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "643a65e0-f163-4df9-a957-5c9bd865f856",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Henning Gamborg Møller\n",
      "Klostergade 28\n",
      "A K Møller\n",
      "Bregnerødvej 75, st. 0002\n",
      "A Møller\n",
      "Violvej 3\n",
      "Ø. Bjerregrav\n",
      "A Møller\n",
      "Hyrdevej 16A\n",
      "A Møller\n",
      "Brammersgade 45\n",
      "A Møller\n",
      "Dalstræde 11\n",
      "Heltborg\n",
      "A Møller\n",
      "Jørgensgaardvej 13\n",
      "A Møller Andersen\n",
      "Gammel Holtevej 60\n",
      "Gl Holte\n",
      "A Møller Jensen\n",
      "Viborgvej 115, 1. TV\n",
      "Hasle\n",
      "A Møller Sørensen\n",
      "Korsørgade 4, 6. tv\n",
      "A Porse Møller\n",
      "Røddikvej 60\n",
      "Aage Beck Møller\n",
      "Rødding Nørregade 8\n",
      "Aage Bojsen-Møller\n",
      "Noret 3\n",
      "Aage Christian Møller Andersen\n",
      "Jordemodervej 7\n",
      "Bislev\n",
      "Aage Hansen Møller\n",
      "Filippavej 38\n",
      "Hundstrup\n",
      "Aage Jan Møller\n",
      "Næsborgvej 50, st. th\n",
      "Aage Karl Møller\n",
      "Dyrevænget 21\n",
      "Tibirke Sand\n",
      "Aage Majbom Møller\n",
      "Pilelunden 23\n",
      "Aage Martin Møller\n",
      "Almind Østergade 15A, 1. tv\n",
      "Aage Møller\n",
      "Ørritslev Gade 7\n",
      "Ørritslev\n",
      "Aage Møller\n",
      "Knudsvej 1\n",
      "Aage Møller\n",
      "Fruevej 4\n",
      "Aage Møller\n",
      "Dybdevej 30\n",
      "Bolbro\n",
      "Aage Møller\n",
      "Hammelmosevej 12\n",
      "Aage Møller\n",
      "Birkevej 6\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "with open('./my_data/addresses.txt') as f:\n",
    "    addresses = f.read()\n",
    "    \n",
    "    \n",
    "    all_street_names= re.compile(r'^([a-zA-ZæøåÆØÅ].+)|(\\D) \\d.+')\n",
    "    #all_street_names= re.compile(r'^(\\w+) ([\\w ]+)*$')\n",
    "    #all_street_names= re.compile(r'^(\\w+) \\d{2}$')\n",
    "    for line in addresses.split('\\n'):\n",
    "        if all_street_names.match(line):\n",
    "            print(all_street_names.match(line).group())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c9b8f2-8c49-469c-afd6-090d45ea12ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Class exercise\n",
    "Find a web site to interact with and fill out a form to get some information back.  \n",
    "Examples could be https://www.jobindex.dk/,    \n",
    "https://google.com or   \n",
    "https://www.ikea.com/dk/da/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79f61496-46ca-4286-bc9a-c516bb32d228",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[1;32m      5\u001b[0m browser \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome()\n",
    "browser.get('https://www.jobindex.dk/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b82f01-e1ca-48c8-af2d-9d406b0ebeae",
   "metadata": {},
   "source": [
    "# Week 7: Web scraping, BS4, Selenium and Regexp\n",
    "### Part 1\n",
    "##### 30 min\n",
    "1. Make a request to: `https://en.wikipedia.org/wiki/Alan_Turing` and print out the responses .text property\n",
    "2. Find and print the title of the response page\n",
    "3. Find and print content of the first p tag that has content.\n",
    "4. Find and print all content from the TOC\n",
    "##### 20 min\n",
    "5. Create a dictionary from the TOC links like: {'first link':'#this_is_the_first_link}\n",
    "### Pause\n",
    "##### 40 min\n",
    "6. Create a function that can take a word and look for it in the dictionary keys and then return the content from the first link that matches. Return the next p elements until next headline (h3 element).\n",
    "7. Make it into a cli program.\n",
    "\n",
    "### Pause\n",
    "\n",
    "### Part 2\n",
    "##### 40 min\n",
    "1. Using regex find out how many times 'Turing' is used in the article\n",
    "9. Regex: Find all the sentences that has a year in them (sentense defined by: starting at \\n or dot or comma and ending at dot or comma.\n",
    "\n",
    "### Part 3\n",
    "##### 40 min\n",
    "1. Use selenium to go to amazon.com and search for `Pet Shower Cap - Waterproof Reusable Bath Ear Covers`\n",
    "2. Print how many products were found\n",
    "3. Find the cheapest and the most expensive product from the list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224e99e1-1014-4134-a07d-79688711c868",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Make a request to: `https://en.wikipedia.org/wiki/Alan_Turing` and print out the responses .text property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d7eff5-44d6-43ae-aa7b-b9611d761042",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs\" dir=\"ltr\" lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <title>\n",
      "   Alan Turing - Wikipedia\n",
      "  </title>\n",
      "  <script>\n",
      "   document.documentElement.className=\"client-js\";RLCONF={\"wgBreakFrames\":false,\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgRequestId\":\"fccbd97d-eeea-454e-ae60-9be77e48c231\",\"wgCSPNonce\":false,\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Alan_Turing\",\"wgTitle\":\"Alan Turing\",\"wgCurRevisionId\":1117844782,\"wgRevisionId\":1117844782,\"wgArticleId\":1208,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"Webarchive template wayback links\",\"CS1: long volume value\",\"Articles with short description\",\"Short description matches Wikidata\",\"Wikipedia indefinitely semi-protected pages\",\"Use British English from June 2020\",\"Use dmy dates from November 2021\",\"Pages using infobox person with multiple partners\",\n",
      "\"Biography with signature\",\"Articles with hCards\",\"Wikipedia references cleanup from April 2022\",\"All articles needing references cleanup\",\"Articles covered by WikiProject Wikify from April 2022\",\"All articles covered by WikiProject Wikify\",\"Wikipedia articles incorporating a citation from the ODNB\",\"Pages using cite ODNB with id parameter\"\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "import requests\n",
    "\n",
    "\n",
    "r = requests.get('https://en.wikipedia.org/wiki/Alan_Turing')\n",
    "r.raise_for_status()\n",
    "soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "print(soup.prettify()[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39feb9ce-59cc-4433-97c7-9f062483d72d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Find and print the title of the response page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ece2cc20-e531-4e0c-a1eb-a1401127bf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alan Turing - Wikipedia\n"
     ]
    }
   ],
   "source": [
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9de4dc-9f4f-4356-8a64-d4e57cb416bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Find and print content of the first p tag that has content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e277ff73-6f55-41f9-9f32-f20cf71cb344",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<span class=\"rt-commentedText nowrap\"><span class=\"IPA nopopups noexcerpt\" lang=\"en-fonipa\"><a href=\"/wiki/Help:IPA/English\" title=\"Help:IPA/English\">/<span style=\"border-bottom:1px dotted\"><span title=\"/ˈ/: primary stress follows\">ˈ</span><span title=\"/tj/: 't' in 'tune'\">tj</span><span title=\"/ʊər/: 'our' in 'tour'\">ʊər</span><span title=\"/ɪ/: 'i' in 'kit'\">ɪ</span><span title=\"/ŋ/: 'ng' in 'sing'\">ŋ</span></span>/</a></span></span>\n"
     ]
    }
   ],
   "source": [
    "elemnts = soup.select('div > p')\n",
    "\n",
    "print(elements[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3236817c-008a-46d6-94ee-bf2fc20965b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Find and print all content from the TOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41af826f-66da-4710-a53b-a3a8279377ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents\n",
      "\n",
      "1 Early life and education\n",
      "\n",
      "1.1 Family\n",
      "1.2 School\n",
      "1.3 Christopher Morcom\n",
      "1.4 University and work on computability\n",
      "\n",
      "\n",
      "2 Career and research\n",
      "\n",
      "2.1 Cryptanalysis\n",
      "2.2 Bombe\n",
      "2.3 Hut 8 and the naval Enigma\n",
      "2.4 Turingery\n",
      "2.5 Delilah\n",
      "2.6 Early computers and the Turing test\n",
      "2.7 Pattern formation and mathematical biology\n",
      "\n",
      "\n",
      "3 Personal life\n",
      "\n",
      "3.1 Engagement\n",
      "3.2 Homosexuality and indecency conviction\n",
      "3.3 Treasure\n",
      "3.4 Death\n",
      "3.5 Government apology and pardon\n",
      "\n",
      "\n",
      "4 Legacy\n",
      "\n",
      "4.1 Awards, honours, and tributes\n",
      "4.2 Centenary celebrations\n",
      "\n",
      "\n",
      "5 Notes and references\n",
      "\n",
      "5.1 Notes\n",
      "5.2 References\n",
      "5.3 Sources\n",
      "\n",
      "\n",
      "6 Further reading\n",
      "\n",
      "6.1 Articles\n",
      "6.2 Books\n",
      "\n",
      "\n",
      "7 External links\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events = soup.select('div[id=toc]')\n",
    "for e in events:\n",
    "    print(e.getText())\n",
    "    #print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5463168c-cb85-427a-9b63-40a388e3e97e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5. Create a dictionary from the TOC links like: {'first link':'#this_is_the_first_link}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "073aa69b-ab71-4e8d-a1e1-bdf0f4b527e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1.1 Family': 'https://en.wikipedia.org/wiki/Alan_Turing#Family',\n",
       " '1.2 School': 'https://en.wikipedia.org/wiki/Alan_Turing#School',\n",
       " '1.3 Christopher Morcom': 'https://en.wikipedia.org/wiki/Alan_Turing#Christopher_Morcom',\n",
       " '1.4 University and work on computability': 'https://en.wikipedia.org/wiki/Alan_Turing#University_and_work_on_computability',\n",
       " '2.1 Cryptanalysis': 'https://en.wikipedia.org/wiki/Alan_Turing#Cryptanalysis',\n",
       " '2.2 Bombe': 'https://en.wikipedia.org/wiki/Alan_Turing#Bombe',\n",
       " '2.3 Hut 8 and the naval Enigma': 'https://en.wikipedia.org/wiki/Alan_Turing#Hut_8_and_the_naval_Enigma',\n",
       " '2.4 Turingery': 'https://en.wikipedia.org/wiki/Alan_Turing#Turingery',\n",
       " '2.5 Delilah': 'https://en.wikipedia.org/wiki/Alan_Turing#Delilah',\n",
       " '2.6 Early computers and the Turing test': 'https://en.wikipedia.org/wiki/Alan_Turing#Early_computers_and_the_Turing_test',\n",
       " '2.7 Pattern formation and mathematical biology': 'https://en.wikipedia.org/wiki/Alan_Turing#Pattern_formation_and_mathematical_biology',\n",
       " '3.1 Engagement': 'https://en.wikipedia.org/wiki/Alan_Turing#Engagement',\n",
       " '3.2 Homosexuality and indecency conviction': 'https://en.wikipedia.org/wiki/Alan_Turing#Homosexuality_and_indecency_conviction',\n",
       " '3.3 Treasure': 'https://en.wikipedia.org/wiki/Alan_Turing#Treasure',\n",
       " '3.4 Death': 'https://en.wikipedia.org/wiki/Alan_Turing#Death',\n",
       " '3.5 Government apology and pardon': 'https://en.wikipedia.org/wiki/Alan_Turing#Government_apology_and_pardon',\n",
       " '4.1 Awards, honours, and tributes': 'https://en.wikipedia.org/wiki/Alan_Turing#Awards,_honours,_and_tributes',\n",
       " '4.2 Centenary celebrations': 'https://en.wikipedia.org/wiki/Alan_Turing#Centenary_celebrations',\n",
       " '5.1 Notes': 'https://en.wikipedia.org/wiki/Alan_Turing#Notes',\n",
       " '5.2 References': 'https://en.wikipedia.org/wiki/Alan_Turing#References',\n",
       " '5.3 Sources': 'https://en.wikipedia.org/wiki/Alan_Turing#Sources',\n",
       " '6.1 Articles': 'https://en.wikipedia.org/wiki/Alan_Turing#Articles',\n",
       " '6.2 Books': 'https://en.wikipedia.org/wiki/Alan_Turing#Books'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ahref = soup.select('.toc > ul > li > ul > li > a')\n",
    "url = 'https://en.wikipedia.org/wiki/Alan_Turing'\n",
    "\n",
    "my_dict = dict()\n",
    "\n",
    "for link in ahref:\n",
    "    my_dict[link.text] = url + link.get('href')\n",
    "    \n",
    "my_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c9a9b4-33f7-4188-abee-d29eee775c29",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 6. Create a function that can take a word and look for it in the dictionary keys and then return the content from the first link that matches. Return the next p elements until next headline (h3 element)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff4f5211-6616-4dd6-9b4f-d877903657ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m______________________\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mHIGHLIGHTED SECTION:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m text)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# -Key \"Career and reserach\"    \u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m getTextFromSection(\u001b[43margs\u001b[49m\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "def getTextFromSection(key):\n",
    "    \n",
    "    for tag in soup.find(\"span\", {\"id\": dict.get(key)[1:]}).find_all_next():\n",
    "        if tag.name == \"p\": text += tag.getText()\n",
    "        elif tag.name == \"h3\": break\n",
    "    print(\"______________________\\n\\nHIGHLIGHTED SECTION:\\n\" + text)\n",
    "\n",
    "# -Key \"Career and reserach\"    \n",
    "getTextFromSection(args.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b153d9-49c4-4fee-8590-eb3ab95ea544",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 7. Make it into a cli program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8c76c6-c4b1-46db-be9a-f4fb85df4a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d152d9e-d182-4cf0-a4b3-de4a798b1a51",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Using regex find out how many times 'Turing' is used in the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a16a0e28-3ef0-408e-ba18-4061ecfdf776",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{438}\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import re\n",
    "\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Alan_Turing'\n",
    "r = requests.get(url) \n",
    "r.raise_for_status()\n",
    "soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "\n",
    "turings = re.compile(r'Turing')\n",
    "\n",
    "\n",
    "output = turings.findall(soup.text)\n",
    "turing_count = 0\n",
    "for turing in output:\n",
    "    turing_count += 1\n",
    "    \n",
    "\n",
    "print({turing_count})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536473be-45c3-4de9-bcbf-674bf26ed41a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 9. Regex: Find all the sentences that has a year in them (sentense defined by: starting at \\n or dot or comma and ending at dot or comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "46ed9186-ee75-4a2a-8afd-f2eead444a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 June 1912\n",
      "7 June 1954\n",
      "d in 1941\n",
      "3 June 1912\n",
      "7 June 1954\n",
      ". In 1938\n",
      ". In 1948\n",
      "n the 1960\n",
      "d in 1952\n",
      "7 June 1954\n",
      "n in 2009\n",
      "n in 2013\n",
      "o a 2017\n",
      "3 June 2021\n",
      ". A 2019\n",
      "1 Oct 1907\n",
      "3 June 1912\n",
      "3 June 2012\n",
      "d in 1927\n",
      "n January 1922\n",
      "] In 1926\n",
      "h the 1926\n",
      "s in 1927\n",
      ". In 1928\n",
      "3 July 1911\n",
      "3 February 1930\n",
      "n February 1930\n",
      "3 February 1933\n",
      "e from 1931\n",
      ". In 1935\n",
      ", in 1922\n",
      "t in 1928\n",
      "e in 1931\n",
      "w in 1935\n",
      "m September 1936\n",
      "o July 1938\n",
      "n June 1938\n",
      "n in 1939\n",
      "m September 1938\n",
      "e July 1939\n",
      "n May 1940\n",
      "e in 1939\n",
      "4 September 1939\n",
      "l April 2012\n",
      "r the 1948\n",
      ") in 1946\n",
      "r of 1019\n",
      ", or 1022\n",
      "8 March 1940\n",
      "y late 1941\n",
      "n December 1939\n",
      "n November 1942\n",
      "n March 1943\n",
      "n July 1942\n",
      "5 and 1947\n",
      "9 February 1946\n",
      "n late 1947\n",
      "0 May 1950\n",
      "n in 1947\n",
      ", October 1950\n",
      ". By 1950\n",
      "] In 1952\n",
      "d in 1951\n",
      "n January 1952\n",
      "e in 1951\n",
      "] In 2012\n",
      "d in 1992\n",
      "n January 1952\n",
      "t Act 1885\n",
      "1 March 1952\n",
      "S in 1946\n",
      "n in 1952\n",
      "n the 1940\n",
      "8 June 1954\n",
      "2 June 1954\n",
      "n August 2009\n",
      "0 September 2009\n",
      "n December 2011\n",
      ". In 1952\n",
      "6 July 2012\n",
      "t Act 1885\n",
      "1 March 1952\n",
      "9 November 2013\n",
      "8 February 2014\n",
      "4 December 2013\n",
      "n August 2014\n",
      "n September 2016\n",
      "e Act 2017\n",
      "r in 2008\n",
      "e in 1946\n",
      ") in 1951\n",
      ". In 1994\n",
      "3 June 2001\n",
      "n Turing 1912\n",
      "3 June 2012\n",
      "5 March 2021\n",
      "e in 2019\n",
      "r in 2012\n",
      "9 December 2017\n",
      "9 December 2017\n",
      ". SETSS 2018\n",
      "3 July 2019\n",
      "9 July 2019\n",
      "9 March 1999\n",
      "9 January 2011\n",
      "0 January 2011\n",
      "^ Sipser 2006\n",
      "^ Beavers 2013\n",
      "8 June 2012\n",
      "1 October 2014\n",
      "6 October 2014\n",
      "8 January 2015\n",
      "7 February 2015\n",
      "9 January 2015\n",
      "0 January 2015\n",
      "9 January 2015\n",
      "1 September 2009\n",
      "3 December 2017\n",
      "7 February 2015\n",
      "3 February 2015\n",
      "9 October 1993\n",
      "^ Leavitt 2007\n",
      "0 October 2016\n",
      "0 October 2016\n",
      "0 October 2016\n",
      "b Hodges 1983\n",
      "4 June 2012\n",
      "2 January 2012\n",
      "3 June 2012\n",
      "h Marriages 1845\n",
      "n ID 1990\n",
      "3 September 2009\n",
      "0 February 2007\n",
      "1 September 2013\n",
      "0 July 2011\n",
      "6 September 2006\n",
      "8 October 2017\n",
      "^ Hodges 1983\n",
      "9 June 2012\n",
      "2 September 2017\n",
      "3 July 2017\n",
      "5 June 2012\n",
      "3 December 2017\n",
      "3 July 2017\n",
      "1 December 2001\n",
      "3 August 2007\n",
      "7 July 2007\n",
      "9 November 2012\n",
      "9 October 2013\n",
      "1 October 2013\n",
      "6 December 2016\n",
      "5 February 2017\n",
      "1 September 2016\n",
      "0 October 2020\n",
      "^ Hodges 1983\n",
      "^ Hodges 1983\n",
      "9 December 2014\n",
      "7 January 2015\n",
      "9 January 2015\n",
      "5 April 2014\n",
      "^ Hodges 1983\n",
      "9 January 2011\n",
      "^ Hodges 1983\n",
      "5 August 2020\n",
      "2 Decembre 2009\n",
      "1 April 2018\n",
      "^ Hodges 1983\n",
      "^ Turing 1937\n",
      "^ Church 1936\n",
      "4 March 2016\n",
      "8 February 2016\n",
      "^ Hodges 1983\n",
      "3 October 2012\n",
      "4 February 2012\n",
      "^ Hodges 1983\n",
      "^ Hodges 1983\n",
      "1 November 2011\n",
      "^ Copeland 2004\n",
      "8 June 2012\n",
      "2 November 2013\n",
      "1 October 2013\n",
      "4 June 2020\n",
      "4 August 2021\n",
      "7 April 2015\n",
      "5 March 2015\n",
      "8 April 2015\n",
      "5 March 2015\n",
      "9 April 2012\n",
      "4 October 2012\n",
      "0 April 2012\n",
      "^ Hodges 1983\n",
      "n late 1940\n",
      "n April 1996\n",
      "^ Lewin 1978\n",
      "9 August 2019\n",
      "9 August 2019\n",
      "0 March 2010\n",
      "1 December 2016\n",
      "0 December 2016\n",
      "4 September 2009\n",
      "8 August 2013\n",
      "3 June 2012\n",
      "4 September 2013\n",
      "2 June 2014\n",
      "0 March 2010\n",
      "8 November 2014\n",
      "2 June 2014\n",
      "3 November 2014\n",
      "2 June 2014\n",
      "9 October 1993\n",
      "3 June 2012\n",
      "7 July 2018\n",
      "1 June 2018\n",
      "6 June 2019\n",
      "6 February 2019\n",
      "8 March 2021\n",
      "^ Oakley 2006\n",
      "b Hodges 1983\n",
      "b Hodges 1983\n",
      "8 February 2015\n",
      "9 July 2007\n",
      "7 July 2007\n",
      "0 June 2007\n",
      "0 June 2007\n",
      "c Mahon 1945\n",
      "^ Leavitt 2007\n",
      ". Fall 1997\n",
      "6 June 2019\n",
      "3 April 2019\n",
      "1 April 2019\n",
      "3 April 2019\n",
      "^ Hodges 1983\n",
      "^ Hodges 1983\n",
      "7 April 2019\n",
      "7 April 2019\n",
      "& circa 1945\n",
      "^ Copeland 2006\n",
      "^ Copeland 2006\n",
      "^ Copeland 2006\n",
      "^ Gannon 2007\n",
      "^ Hilton 2006\n",
      "^ Copeland 2006\n",
      "^ Hodges 1983\n",
      "^ Hodges 1983\n",
      "^ Hodges 1983\n",
      "^ Copeland 2006\n",
      "7 January 2012\n",
      "7 January 2012\n",
      "3 February 1946\n",
      "e Copeland 2004\n",
      "5 July 2015\n",
      "3 July 2015\n",
      "n in 1947\n",
      "1 May 2013\n",
      "7 February 2013\n",
      "7 February 2019\n",
      "8 March 2019\n",
      "8 October 2017\n",
      "2 November 2013\n",
      "1 November 2013\n",
      "9 February 2006\n",
      "6 April 2017\n",
      "9 October 2017\n",
      "2 May 2018\n",
      "4 August 1952\n",
      "3 August 2003\n",
      "8 November 2011\n",
      ", March 1988\n",
      "5 September 2015\n",
      "7 July 2015\n",
      "^ Leavitt 2007\n",
      "^ Hodges 1983\n",
      "^ Leavitt 2007\n",
      "3 June 2022\n",
      "0 January 2013\n",
      "6 December 2012\n",
      "^ Hodges 1983\n",
      "^ Copeland 2006\n",
      "3 July 2021\n",
      "1 October 2017\n",
      "1 October 2017\n",
      "^ Hodges 1983\n",
      "^ Leavitt 2007\n",
      "d Hodges 1983\n",
      "^ Hodges 1983\n",
      "7 January 2019\n",
      "6 January 2019\n",
      "3 June 2012\n",
      "3 June 2012\n",
      "3 June 2012\n",
      "8 July 1954\n",
      "1 June 1971\n",
      "t before 1959\n",
      "6 May 1955\n",
      "7 February 2019\n",
      "6 February 2019\n",
      "^ Hodges 1983\n",
      "6 June 2014\n",
      "7 January 2019\n",
      "6 January 2019\n",
      "1 August 2009\n",
      "1 August 2009\n",
      "1 August 2009\n",
      "1 September 2009\n",
      "5 October 2009\n",
      "1 September 2009\n",
      "1 September 2009\n",
      "4 February 2017\n",
      "0 December 2016\n",
      "1 September 2009\n",
      "1 May 2012\n",
      "1 September 2009\n",
      "9 November 2012\n",
      "6 December 2011\n",
      "0 January 2012\n",
      "6 December 2011\n",
      "9 June 2018\n",
      "1 June 2018\n",
      "3 December 2013\n",
      "4 December 2013\n",
      "1 August 2017\n",
      "7 February 2012\n",
      "4 February 2017\n",
      "0 December 2016\n",
      "2 February 2012\n",
      "6 July 2017\n",
      "9 August 2017\n",
      "4 December 2013\n",
      "6 August 2016\n",
      "5 September 2016\n",
      "4 September 2016\n",
      "5 September 2016\n",
      "5 September 2016\n",
      "4 December 2013\n",
      "4 June 2018\n",
      "4 June 2018\n",
      "1 February 2012\n",
      "2 July 2016\n",
      "5 September 2016\n",
      "4 June 2018\n",
      "4 June 2018\n",
      "5 July 2019\n",
      "9 July 2019\n",
      "9 July 2019\n",
      "0 October 2016\n",
      "5 July 2019\n",
      "3 October 2016\n",
      "1 January 2017\n",
      "5 July 2019\n",
      "6 July 2012\n",
      "2 November 2013\n",
      "1 October 2013\n",
      "4 February 2017\n",
      "3 December 2012\n",
      "5 December 2012\n",
      "9 July 2013\n",
      "4 January 2017\n",
      "0 December 2016\n",
      "0 October 2013\n",
      "4 December 2013\n",
      "4 December 2013\n",
      "5 July 2013\n",
      "0 July 2013\n",
      "2 December 2013\n",
      "5 December 2013\n",
      "4 December 2013\n",
      "2 December 2013\n",
      "2 June 2018\n",
      "0 June 2018\n",
      "2 November 2013\n",
      "4 December 2013\n",
      "3 December 2013\n",
      "2 May 2018\n",
      "5 April 2018\n",
      "4 December 2013\n",
      "4 December 2013\n",
      "4 December 2013\n",
      "2 August 2014\n",
      "1 November 2014\n",
      "1 November 2014\n",
      "3 January 2014\n",
      "6 January 2014\n",
      "4 December 2013\n",
      "2 September 2016\n",
      "2 September 2016\n",
      "2 September 2016\n",
      "1 September 2016\n",
      "2 September 2016\n",
      "2 September 2016\n",
      "e Act 2017\n",
      "5 March 2019\n",
      "6 February 2019\n",
      "7 February 2007\n",
      "4 November 2013\n",
      "7 December 2014\n",
      "7 December 2018\n",
      "8 December 2018\n",
      "9 December 2018\n",
      "8 December 2018\n",
      "1 May 2020\n",
      "8 May 2020\n",
      "5 March 2021\n",
      "5 March 2021\n",
      ", München 2012\n",
      "n Copeland 2006\n",
      "t Eight 1939\n",
      "0 December 2009\n",
      ": July 1939\n",
      "– August 1945\n",
      "y November 1936\n",
      "d in 1959\n",
      "s in 2012\n",
      ". This 1986\n",
      "n a 1997\n",
      "l Laboratory 1945\n",
      "0 January 2011\n",
      "9 March 1999\n",
      "6 October 2007\n",
      "d in 1983\n",
      "d in 1959\n",
      "1 December 2001\n",
      "3 August 2007\n",
      "d in 1951\n",
      "m June 2020\n",
      "m November 2021\n",
      "m April 2022\n",
      "m April 2022\n",
      "5 September 2022\n"
     ]
    }
   ],
   "source": [
    "sentence_with_year = re.compile(r'^\\\\n|. \\w+ \\d{4,4}')\n",
    "year_output =  sentence_with_year.findall(soup.text)\n",
    "for sentence in year_output:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baa725e-d9c0-4f65-8ef4-5677625ed2c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### . Use selenium to go to amazon.com and search for `Pet Shower Cap - Waterproof Reusable Bath Ear Covers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584ef3c3-0fa9-4f63-87ee-faef19a411d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68e8d6b7-3e66-48b3-9562-4e340d509206",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Print how many products were found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd6813c-ba67-4c47-a8e0-950d218a6df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f4cc654-fdc7-4d54-9309-e508ff482e32",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Find the cheapest and the most expensive product from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09f9ae7-c2c5-47d5-9f7b-db7166cc0076",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
